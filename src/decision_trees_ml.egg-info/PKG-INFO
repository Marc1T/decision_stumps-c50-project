Metadata-Version: 2.4
Name: decision-trees-ml
Version: 1.0.0
Summary: Impl√©mentation compl√®te de Decision Stumps et C5.0 from scratch
Home-page: https://github.com/votre-username/decision_stumps_c50
Author: √âquipe ENSAM Mekn√®s
Author-email: votre.email@example.com
License: MIT
Keywords: machine-learning decision-trees decision-stumps c50 c4.5 classification ensemble-learning adaboost boosting
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Operating System :: OS Independent
Classifier: Natural Language :: French
Classifier: Natural Language :: English
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy<2.0.0,>=1.21.0
Requires-Dist: pandas<3.0.0,>=1.3.0
Requires-Dist: scikit-learn<2.0.0,>=1.0.0
Requires-Dist: matplotlib<4.0.0,>=3.4.0
Requires-Dist: seaborn<1.0.0,>=0.11.0
Requires-Dist: graphviz>=0.20.0
Requires-Dist: plotly<6.0.0,>=5.0.0
Requires-Dist: jupyter>=1.0.0
Requires-Dist: notebook<8.0.0,>=6.4.0
Requires-Dist: ipywidgets<9.0.0,>=7.6.0
Requires-Dist: ipykernel>=6.0.0
Requires-Dist: flask<4.0.0,>=2.0.0
Requires-Dist: fastapi>=0.95.0
Requires-Dist: uvicorn>=0.20.0
Requires-Dist: streamlit>=1.20.0
Requires-Dist: gunicorn>=20.1.0
Requires-Dist: joblib>=1.1.0
Requires-Dist: tqdm>=4.62.0
Requires-Dist: python-dotenv>=0.19.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: openpyxl>=3.0.9
Requires-Dist: xlrd>=2.0.1
Requires-Dist: numba>=0.55.0
Requires-Dist: cython>=0.29.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=3.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints>=1.12.0; extra == "docs"
Provides-Extra: visualization
Requires-Dist: graphviz>=0.20.0; extra == "visualization"
Requires-Dist: plotly>=5.0.0; extra == "visualization"
Requires-Dist: seaborn>=0.11.0; extra == "visualization"
Provides-Extra: deployment
Requires-Dist: flask>=2.0.0; extra == "deployment"
Requires-Dist: fastapi>=0.95.0; extra == "deployment"
Requires-Dist: uvicorn>=0.20.0; extra == "deployment"
Requires-Dist: streamlit>=1.20.0; extra == "deployment"
Requires-Dist: gunicorn>=20.1.0; extra == "deployment"
Provides-Extra: notebooks
Requires-Dist: jupyter>=1.0.0; extra == "notebooks"
Requires-Dist: notebook>=6.4.0; extra == "notebooks"
Requires-Dist: ipywidgets>=7.6.0; extra == "notebooks"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Decision Stumps & C5.0 Implementation üå≥

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)]()
[![Coverage](https://img.shields.io/badge/coverage-90%2B-brightgreen.svg)]()

Impl√©mentation compl√®te from scratch des **Decision Stumps** (souches de d√©cision) et de l'algorithme **C5.0** avec tous les fondements math√©matiques, les algorithmes d√©taill√©s et une library Python production-ready.

## üìö Table des Mati√®res

- [Pr√©sentation](#pr√©sentation)
- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Installation](#installation)
- [Utilisation Rapide](#utilisation-rapide)
- [Documentation](#documentation)
- [Architecture](#architecture)
- [Exemples](#exemples)
- [Tests](#tests)
- [Benchmarks](#benchmarks)
- [D√©ploiement](#d√©ploiement)
- [Contributeurs](#contributeurs)
- [Licence](#licence)

## üéØ Pr√©sentation

Ce projet impl√©mente deux algorithmes fondamentaux d'apprentissage automatique pour la classification :

### Decision Stumps (Souches de D√©cision)
- Arbres de d√©cision de **profondeur 1** (un seul niveau)
- Classifieurs faibles ultra-rapides
- Utilis√©s principalement dans les m√©thodes d'ensemble (AdaBoost, Gradient Boosting)
- Complexit√© : O(dn log n) entra√Ænement, O(1) pr√©diction

### C5.0
- √âvolution moderne de l'algorithme C4.5 de Ross Quinlan (1993)
- Utilise le **Gain Ratio** pour √©viter le biais vers attributs multi-valu√©s
- **√âlagage par erreur** pour r√©duire le surapprentissage
- Gestion native des **valeurs manquantes**
- Support du **boosting** int√©gr√©
- 10√ó plus rapide que C4.5 avec meilleure pr√©cision

## ‚ú® Fonctionnalit√©s

### Core Features
- ‚úÖ **Decision Stump** avec crit√®res Gini, Entropie, Erreur de classification
- ‚úÖ **C5.0 Tree** avec Gain Ratio et √©lagage pessimiste
- ‚úÖ **Gestion des valeurs manquantes** (distribution probabiliste)
- ‚úÖ **Boosting** natif (AdaBoost style)
- ‚úÖ **Pond√©ration des co√ªts** d'erreur
- ‚úÖ **Extraction de r√®gles** IF-THEN depuis les arbres

### Compatibilit√© scikit-learn
- ‚úÖ Interface standard `fit()`, `predict()`, `predict_proba()`
- ‚úÖ Compatible avec `cross_val_score`, `GridSearchCV`
- ‚úÖ Int√©grable dans des `Pipeline`
- ‚úÖ Attributs standardis√©s (`feature_importances_`, etc.)

### Visualisation
- ‚úÖ Graphiques d'arbres (Graphviz, Matplotlib)
- ‚úÖ Courbes ROC, matrices de confusion
- ‚úÖ Importance des features
- ‚úÖ Visualisation des d√©cisions

### D√©ploiement
- ‚úÖ API REST (Flask/FastAPI)
- ‚úÖ Interface web interactive (Streamlit)
- ‚úÖ Containerisation Docker
- ‚úÖ Export de mod√®les (pickle, joblib, ONNX)

## üöÄ Installation

### Installation via pip (recommand√©)

```bash
pip install decision-trees-ml
```

### Installation depuis les sources

```bash
# Cloner le repository
git clone https://github.com/votre-repo/decision_stumps_c50.git
cd decision_stumps_c50_project

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Installer le package en mode d√©veloppement
pip install -e .
```

### D√©pendances

```
numpy >= 1.21.0
pandas >= 1.3.0
scikit-learn >= 1.0.0
matplotlib >= 3.4.0
```

## üí° Utilisation Rapide

### Decision Stump

```python
from decision_trees_ml import DecisionStump
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Charger donn√©es
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Cr√©er et entra√Æner un Decision Stump
stump = DecisionStump(criterion='gini')
stump.fit(X_train, y_train)

# Pr√©dire
y_pred = stump.predict(X_test)

# √âvaluer
accuracy = stump.score(X_test, y_test)
print(f"Accuracy: {accuracy:.2%}")

# Voir les param√®tres du stump
print(f"Feature: {stump.feature_index_}")
print(f"Threshold: {stump.threshold_:.2f}")
```

### C5.0 Tree

```python
from decision_trees_ml import C50Tree

# Cr√©er un arbre C5.0
tree = C50Tree(
    max_depth=10,
    min_samples_split=2,
    pruning=True,
    boosting_rounds=0
)

# Entra√Æner
tree.fit(X_train, y_train, feature_names=['sepal_length', 'sepal_width', 
                                           'petal_length', 'petal_width'])

# Pr√©dire avec probabilit√©s
y_pred = tree.predict(X_test)
y_proba = tree.predict_proba(X_test)

# √âvaluer
accuracy = tree.score(X_test, y_test)
print(f"C5.0 Accuracy: {accuracy:.2%}")

# Extraire r√®gles
rules = tree.to_rules()
print(rules)

# Importance des features
importances = tree.feature_importances_
for name, imp in zip(feature_names, importances):
    print(f"{name}: {imp:.4f}")
```

### AdaBoost avec Decision Stumps

```python
from decision_trees_ml import AdaBoostStump

# Cr√©er ensemble AdaBoost
ada = AdaBoostStump(n_estimators=50)
ada.fit(X_train, y_train)

# Pr√©dire
y_pred = ada.predict(X_test)
accuracy = ada.score(X_test, y_test)
print(f"AdaBoost Accuracy: {accuracy:.2%}")
```

### C5.0 avec Boosting Int√©gr√©

```python
# C5.0 avec boosting (comme impl√©mentation originale)
boosted_tree = C50Tree(
    max_depth=5,
    pruning=True,
    boosting_rounds=10  # 10 it√©rations de boosting
)

boosted_tree.fit(X_train, y_train)
accuracy = boosted_tree.score(X_test, y_test)
print(f"C5.0 Boosted Accuracy: {accuracy:.2%}")
```

## üìñ Documentation

### Documentation Compl√®te

- üìÑ **[Rapport LaTeX](docs/rapport/main.pdf)** : 40+ pages de fondements math√©matiques
- üìì **[Notebooks Tutoriels](examples/notebooks/)** : 5 tutoriels interactifs
- üåê **[API Documentation](docs/api/index.html)** : Documentation auto-g√©n√©r√©e
- üéì **[Pr√©sentation](docs/presentation/slides.pdf)** : Slides du projet

### Tutoriels Jupyter

1. **[Introduction](examples/notebooks/01_introduction.ipynb)** : Vue d'ensemble
2. **[Decision Stump Tutorial](examples/notebooks/02_decision_stump_tutorial.ipynb)** : Guide complet DS
3. **[C5.0 Tutorial](examples/notebooks/03_c50_tutorial.ipynb)** : Guide complet C5.0
4. **[Mathematical Details](examples/notebooks/04_mathematical_details.ipynb)** : D√©tails math√©matiques
5. **[Full Pipeline](examples/notebooks/05_full_pipeline.ipynb)** : Pipeline ML complet

## üèóÔ∏è Architecture

```
decision_stumps_c50_project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ decision_stump/    # Module Decision Stump
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stump.py       # Classe principale
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ criteria.py    # Gini, Entropie, Erreur
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py       # Utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ c50/               # Module C5.0
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tree.py        # Classe C50Tree
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node.py        # Structure de n≈ìud
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ splitter.py    # Gain Ratio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruning.py     # √âlagage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ missing_values.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ boosting.py
‚îÇ   ‚îú‚îÄ‚îÄ ensemble/          # M√©thodes d'ensemble
‚îÇ   ‚îú‚îÄ‚îÄ visualization/     # Visualisation
‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Utilitaires g√©n√©raux
‚îú‚îÄ‚îÄ tests/                 # Tests unitaires
‚îú‚îÄ‚îÄ examples/              # Exemples d'utilisation
‚îú‚îÄ‚îÄ benchmarks/            # Comparaisons de performance
‚îî‚îÄ‚îÄ deployment/            # API, Streamlit, Docker
```

## üìä Exemples

Le dossier `examples/` contient 5 scripts progressifs :

```bash
# 1. Exemple basique Decision Stump
python examples/01_basic_decision_stump.py

# 2. Classification avec C5.0
python examples/02_c50_classification.py

# 3. Ensemble AdaBoost
python examples/03_adaboost_ensemble.py

# 4. Comparaison DS vs C5.0 vs sklearn
python examples/04_comparison.py

# 5. Application sur donn√©es r√©elles
python examples/05_real_world_dataset.py
```

## üß™ Tests

Le projet inclut une suite de tests compl√®te avec coverage > 90%.

```bash
# Lancer tous les tests
pytest tests/ -v

# Avec coverage
pytest tests/ --cov=src --cov-report=html

# Tests sp√©cifiques
pytest tests/test_decision_stump.py -v
pytest tests/test_c50.py -v
```

### Tests Disponibles

- ‚úÖ `test_decision_stump.py` : Tests unitaires DS
- ‚úÖ `test_c50.py` : Tests unitaires C5.0
- ‚úÖ `test_criteria.py` : Tests crit√®res d'impuret√©
- ‚úÖ `test_pruning.py` : Tests √©lagage
- ‚úÖ `test_boosting.py` : Tests boosting
- ‚úÖ `test_integration.py` : Tests d'int√©gration

## ‚ö° Benchmarks

Comparaisons de performance sur plusieurs datasets :

```bash
# Benchmark vitesse
python benchmarks/speed_comparison.py

# Benchmark pr√©cision
python benchmarks/accuracy_comparison.py
```

### R√©sultats Typiques (Iris Dataset)

| Algorithme | Accuracy | Temps Entra√Ænement | Temps Pr√©diction |
|------------|----------|-------------------|------------------|
| Decision Stump | 66.7% | 0.001s | 0.0001s |
| C5.0 | 95.6% | 0.015s | 0.001s |
| C5.0 + Boosting | 97.8% | 0.12s | 0.005s |
| sklearn DecisionTree | 95.6% | 0.002s | 0.0001s |

## üöÄ D√©ploiement

### API REST

```bash
cd deployment/api
pip install -r requirements_api.txt
python app.py
```

Acc√®s : `http://localhost:5000`

Endpoints :
- `POST /predict` : Pr√©dire classes
- `POST /train` : Entra√Æner mod√®le
- `GET /model/info` : Informations sur le mod√®le

### Application Streamlit

```bash
cd deployment/streamlit
streamlit run app.py
```

Interface interactive pour :
- Entra√Æner des mod√®les
- Visualiser les arbres
- Tester sur donn√©es personnalis√©es
- Comparer algorithmes

### Docker

```bash
cd deployment/docker
docker-compose up
```

## üë• Contributeurs

Ce projet a √©t√© r√©alis√© dans le cadre d'un projet acad√©mique √† l'ENSAM Mekn√®s :

- **Nankouli Marc Thierry**
- **El Khatar Saad**
- **El Filali**

**Encadrant :** Mr. Idriss Barbara

## üìú Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üìö R√©f√©rences

- Quinlan, J.R. (1993). *C4.5: Programs for Machine Learning*. Morgan Kaufmann.
- Quinlan, J.R. (1996). *Improved Use of Continuous Attributes in C4.5*. JAIR, 4:77-90.
- Freund, Y., Schapire, R.E. (1997). *A Decision-Theoretic Generalization of On-Line Learning*. JCSS, 55(1):119-139.
- Hastie, T., Tibshirani, R., Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

## ü§ù Contributions

Les contributions sont les bienvenues ! N'h√©sitez pas √† :

1. Fork le projet
2. Cr√©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## üìß Contact

<!-- Pour toute question ou suggestion : [votre.email@example.com](mailto:votre.email@example.com) -->

---

‚≠ê Si ce projet vous a √©t√© utile, n'oubliez pas de mettre une √©toile !
