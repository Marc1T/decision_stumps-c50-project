# Decision Stump & C5.0 Stump - Implementation from Scratch ğŸŒ³

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://img.shields.io/badge/tests-58%20passed-brightgreen.svg)]()
[![Code Quality](https://img.shields.io/badge/code%20quality-A-brightgreen.svg)]()

> **ImplÃ©mentation complÃ¨te from scratch de Decision Stumps et C5.0 Stumps**  
> Projet acadÃ©mique - ENSAM MeknÃ¨s 2024-2025

---

## ğŸ“– PrÃ©sentation

Ce projet implÃ©mente deux algorithmes fondamentaux d'apprentissage automatique :

### ğŸ”µ Decision Stump (Souche de DÃ©cision)
- Arbre de dÃ©cision de **profondeur 1** (classifieur le plus simple)
- 3 critÃ¨res d'impuretÃ© : **Gini**, **Entropie**, **Erreur de classification**
- UtilisÃ© comme classifieur faible dans **AdaBoost** et **Gradient Boosting**
- ComplexitÃ© : **O(dn log n)** entraÃ®nement, **O(1)** prÃ©diction

### ğŸŸ¢ C5.0 Stump (Version OptimisÃ©e)
- Version avancÃ©e avec optimisations de **C5.0** (successeur de C4.5)
- **Gain Ratio** (correction du biais du Gain d'Information)
- Gestion native des **valeurs manquantes** (distribution probabiliste)
- **Ã‰lagage pessimiste** pour meilleure gÃ©nÃ©ralisation
- Support de **matrices de coÃ»ts** asymÃ©triques
- Statistiques dÃ©taillÃ©es pour analyse

---

## âœ¨ FonctionnalitÃ©s Principales

### Decision Stump
âœ… 3 critÃ¨res d'impuretÃ© (Gini, Entropie, Erreur)  
âœ… Support des poids d'Ã©chantillons  
âœ… Compatible scikit-learn  
âœ… Ultra-rapide (< 1ms sur 1000 exemples)  
âœ… Parfait pour ensembles (AdaBoost)  

### C5.0 Stump
âœ… **Gain Ratio** (Ã©vite biais attributs multi-valuÃ©s)  
âœ… **Valeurs manquantes** (gestion probabiliste native)  
âœ… **Ã‰lagage** (pessimistic error-based pruning)  
âœ… **CoÃ»ts asymÃ©triques** (matrice de coÃ»ts personnalisÃ©e)  
âœ… **Statistiques** (entropie, gain, erreur, etc.)  
âœ… Documentation dÃ©taillÃ©e  

---

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- pip

### Installation en mode dÃ©veloppement

```bash
# Cloner le projet
cd decision_stumps_c50_project

# CrÃ©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Installer dÃ©pendances
pip install -r requirements.txt

# Installer en mode dev
python install_dev.py
```

### VÃ©rification

```bash
# Test rapide
python quick_test_c50.py

# Tests unitaires
pytest tests/ -v

# Exemples
python examples/01_basic_decision_stump.py
python examples/02_c50_stump_comparison.py
```

---

## ğŸ’¡ Utilisation Rapide

### Decision Stump

```python
from decision_stump import DecisionStump
import numpy as np

# DonnÃ©es
X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([0, 0, 0, 1, 1, 1])

# CrÃ©er et entraÃ®ner
stump = DecisionStump(criterion='gini')
stump.fit(X, y)

# PrÃ©dire
y_pred = stump.predict(X)
print(f"Accuracy: {stump.score(X, y):.2%}")

# Afficher
print(stump)
# Decision Stump:
#   IF feature[0] <= 3.5000:
#     PREDICT class 0
#   ELSE:
#     PREDICT class 1
#   Gain: 0.5000
```

### C5.0 Stump

```python
from c50 import C50Stump
import numpy as np

# DonnÃ©es avec valeurs manquantes
X = np.array([[1.0], [2.0], [np.nan], [4.0], [5.0], [6.0]])
y = np.array([0, 0, 0, 1, 1, 1])

# CrÃ©er avec gestion NaN et Ã©lagage
stump = C50Stump(
    handle_missing=True,
    use_pruning=True,
    confidence_level=0.25
)

# EntraÃ®ner
stump.fit(X, y)

# PrÃ©dire (gÃ¨re automatiquement les NaN)
y_pred = stump.predict(X)

# Statistiques
print(stump.stats_)
# {'n_samples': 6, 'n_features': 1, 'n_classes': 2,
#  'initial_entropy': 1.0, 'final_gain_ratio': 0.918,
#  'error_rate': 0.0, 'is_pruned': False}
```

## ğŸ§ª Tests

Le projet inclut **58 tests unitaires** avec 100% de rÃ©ussite.

```bash
# Tous les tests
pytest tests/ -v

# Avec coverage
pytest tests/ --cov=src --cov-report=html

# Tests spÃ©cifiques
pytest tests/test_decision_stump.py -v  # 32 tests
pytest tests/test_c50_stump.py -v       # 26 tests
```

### Couverture des Tests

- âœ… CritÃ¨res d'impuretÃ© (Gini, Entropie, Erreur)
- âœ… EntraÃ®nement et prÃ©diction
- âœ… Gestion des valeurs manquantes
- âœ… Ã‰lagage pessimiste
- âœ… Matrice de coÃ»ts
- âœ… Cas limites (donnÃ©es vides, une seule classe, etc.)
- âœ… CompatibilitÃ© sklearn
- âœ… Poids des Ã©chantillons

---

## ğŸ“ Exemples

### Exemple 1 : Utilisation Basique
```bash
python examples/01_basic_decision_stump.py
```

DÃ©montre :
- EntraÃ®nement sur donnÃ©es simples
- Comparaison des 3 critÃ¨res (Gini, Entropie, Erreur)
- Ã‰chantillons pondÃ©rÃ©s
- Visualisations

### Exemple 2 : Comparaison Decision Stump vs C5.0 Stump
```bash
python examples/02_c50_stump_comparison.py
```

DÃ©montre :
1. **Gain Ratio** corrige le biais du Gain d'Information
2. **Gestion des valeurs manquantes** (NaN)
3. **Ã‰lagage pessimiste**
4. **Matrice de coÃ»ts** asymÃ©triques
5. **Benchmark complet** sur dataset rÃ©el

---

## ğŸ“š Documentation

### Structure du Projet

```
decision_stumps_c50_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ decision_stump/         # Module Decision Stump
â”‚   â”‚   â”œâ”€â”€ stump.py           # Classe principale
â”‚   â”‚   â””â”€â”€ criteria.py        # CritÃ¨res d'impuretÃ©
â”‚   â””â”€â”€ c50/                   # Module C5.0 Stump
â”‚       â”œâ”€â”€ stump.py           # Classe principale
â”‚       â””â”€â”€ README_C50_STUMP.md # Doc dÃ©taillÃ©e
â”‚
â”œâ”€â”€ tests/                      # Tests unitaires (58 tests)
â”‚   â”œâ”€â”€ test_decision_stump.py
â”‚   â””â”€â”€ test_c50_stump.py
â”‚
â”œâ”€â”€ examples/                   # Exemples d'utilisation
â”‚   â”œâ”€â”€ 01_basic_decision_stump.py
â”‚   â””â”€â”€ 02_c50_stump_comparison.py
â”‚
â””â”€â”€ docs/
    â””â”€â”€ rapport/
        â””â”€â”€ main.tex           # Rapport LaTeX complet
```

### Documentation DÃ©taillÃ©e

- ğŸ“„ **[README_C50_STUMP.md](src/c50/README_C50_STUMP.md)** : Guide complet C5.0 Stump
- ğŸ“„ **[Rapport LaTeX](docs/rapport/main.tex)** : 40+ pages de fondements mathÃ©matiques
- ğŸ“„ **Docstrings** : Toutes les fonctions documentÃ©es (format Google)

---

## ğŸ”¬ Fondements MathÃ©matiques

### Gain Ratio (C5.0)

```
Gain Ratio = Information Gain / Split Info

oÃ¹:
  Information Gain = H(S) - Î£ (|Sáµ¢|/|S|) Ã— H(Sáµ¢)
  Split Info = -Î£ (|Sáµ¢|/|S|) Ã— logâ‚‚(|Sáµ¢|/|S|)
```

### Ã‰lagage Pessimiste

```
error_rate = (E + 0.5) / (N + 1)  [Laplace smoothing]

pessimistic_error = error_rate + z Ã— âˆš(error_rate Ã— (1-error_rate) / N)

Si error(feuille) â‰¤ error(stump) â†’ Ã©laguer
```

### Valeurs Manquantes

```
Pour attribut A avec seuil Î¸:
1. Calculer division sur valeurs valides
2. p_left = |S_left| / |S_valid|
   p_right = |S_right| / |S_valid|
3. Pour x avec A=NaN:
   Assigner Ã  gauche avec probabilitÃ© p_left
```

---

## ğŸ‘¥ Contributeurs

**Ã‰quipe ENSAM MeknÃ¨s 2025-2026**

- **Nankouli Marc Thierry**
- **El Khatar Saad**
- **El Filali**

**Encadrant :** Pr Hosni

---

## ğŸ“œ Licence

Ce projet est sous licence MIT. Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- **Ross Quinlan** pour les algorithmes C4.5 et C5.0
- **Yoav Freund & Robert Schapire** pour AdaBoost
- **ENSAM MeknÃ¨s** pour le cadre du projet

---

## ğŸ“š RÃ©fÃ©rences

1. Quinlan, J.R. (1993). *C4.5: Programs for Machine Learning*. Morgan Kaufmann.
2. Quinlan, J.R. (1996). *Improved Use of Continuous Attributes in C4.5*. JAIR, 4:77-90.
3. Breiman, L. et al. (1984). *Classification and Regression Trees*. Wadsworth.
4. Hastie, T., Tibshirani, R., Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

---
<!-- 
## ğŸ“ Contact

Pour toute question ou suggestion : [GitHub Issues](https://github.com/votre-repo/issues) -->

---

â­ **Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  lui donner une Ã©toile !**