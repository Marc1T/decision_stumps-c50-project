"""
Exemple 2 : Comparaison Decision Stump vs C5.0 Stump

Ce script dÃ©montre les amÃ©liorations du C5.0 Stump:
1. Gain Ratio vs Gain d'Information (correction du biais)
2. Gestion des valeurs manquantes
3. Ã‰lagage pessimiste
4. Matrice de coÃ»ts
5. Comparaison des performances

Auteur: Ã‰quipe ENSAM MeknÃ¨s
Date: 2024-2025
"""

import numpy as np
import matplotlib.pyplot as plt
import sys
sys.path.insert(0, 'src')

from decision_stump import DecisionStump
from c50.stump import C50Stump


def example_1_gain_ratio_bias():
    """Exemple 1: Correction du biais par Gain Ratio."""
    print("="*70)
    print("EXEMPLE 1: GAIN RATIO CORRIGE LE BIAIS")
    print("="*70 + "\n")
    
    # Dataset conÃ§u pour montrer le biais du Gain d'Information
    # Feature 0: Beaucoup de valeurs distinctes (ID-like)
    # Feature 1: Peu de valeurs mais informative
    np.random.seed(42)
    
    n = 100
    # Feature 0: ID quasi-unique (biais pour Gain d'Info)
    X_feat0 = np.arange(n) + np.random.randn(n) * 0.1
    
    # Feature 1: Vraiment informative (2 valeurs)
    X_feat1 = np.repeat([0, 1], n//2)
    
    X = np.column_stack([X_feat0, X_feat1])
    y = np.repeat([0, 1], n//2)  # Parfaitement corrÃ©lÃ© avec feature 1
    
    print("ğŸ“Š Dataset artificiel:")
    print(f"  - Feature 0: {len(np.unique(X_feat0))} valeurs distinctes (quasi-ID)")
    print(f"  - Feature 1: {len(np.unique(X_feat1))} valeurs distinctes (informative)")
    print(f"  - Target parfaitement corrÃ©lÃ© avec Feature 1\n")
    
    # Decision Stump classique (avec Entropie = Gain d'Information)
    print("ğŸ”µ DECISION STUMP CLASSIQUE (Gain d'Information):")
    print("â”€"*70)
    ds = DecisionStump(criterion='entropy')
    ds.fit(X, y)
    print(f"  Feature sÃ©lectionnÃ©e: {ds.feature_index_}")
    print(f"  Seuil: {ds.threshold_:.4f}")
    print(f"  Accuracy: {ds.score(X, y):.2%}\n")
    
    # C5.0 Stump (avec Gain Ratio)
    print("ğŸŸ¢ C5.0 STUMP (Gain Ratio):")
    print("â”€"*70)
    c50 = C50Stump()
    c50.fit(X, y)
    print(f"  Feature sÃ©lectionnÃ©e: {c50.feature_index_}")
    print(f"  Seuil: {c50.threshold_:.4f}")
    print(f"  Gain Ratio: {c50.gain_ratio_:.4f}")
    print(f"  Information Gain: {c50.information_gain_:.4f}")
    print(f"  Split Info: {c50.split_info_:.4f}")
    print(f"  Accuracy: {c50.score(X, y):.2%}\n")
    
    print("ğŸ’¡ InterprÃ©tation:")
    print("  - Gain d'Information favorise les attributs avec beaucoup de valeurs")
    print("  - Gain Ratio pÃ©nalise cette tendance via Split Info")
    print("  - C5.0 devrait choisir Feature 1 (plus simple et tout aussi prÃ©cise)\n")


def example_2_missing_values():
    """Exemple 2: Gestion des valeurs manquantes."""
    print("\n" + "="*70)
    print("EXEMPLE 2: GESTION DES VALEURS MANQUANTES")
    print("="*70 + "\n")
    
    # Dataset avec valeurs manquantes
    X_train = np.array([
        [1.0, 10.0],
        [2.0, 20.0],
        [np.nan, 30.0],  # Valeur manquante
        [4.0, np.nan],    # Valeur manquante
        [5.0, 50.0],
        [6.0, 60.0]
    ])
    y_train = np.array([0, 0, 0, 1, 1, 1])
    
    print("ğŸ“Š DonnÃ©es d'entraÃ®nement avec valeurs manquantes (NaN):")
    print("X_train:")
    for i, (x, label) in enumerate(zip(X_train, y_train)):
        print(f"  Exemple {i}: {x} â†’ classe {label}")
    print()
    
    # Decision Stump classique (ne gÃ¨re pas NaN nativement)
    print("ğŸ”µ DECISION STUMP CLASSIQUE:")
    print("â”€"*70)
    try:
        ds = DecisionStump()
        ds.fit(X_train, y_train)
        print(f"  âœ… EntraÃ®nÃ© (ignore probablement les NaN)")
        print(f"  Feature: {ds.feature_index_}, Seuil: {ds.threshold_:.2f}")
        
        # Test avec NaN
        X_test = np.array([[2.5, 25.0], [np.nan, np.nan]])
        y_pred = ds.predict(X_test)
        print(f"  PrÃ©dictions: {y_pred}")
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
    
    print()
    
    # C5.0 Stump (gÃ¨re NaN nativement)
    print("ğŸŸ¢ C5.0 STUMP:")
    print("â”€"*70)
    c50 = C50Stump(handle_missing=True)
    c50.fit(X_train, y_train)
    
    print(f"  âœ… EntraÃ®nÃ© avec gestion des valeurs manquantes")
    print(f"  Feature: {c50.feature_index_}, Seuil: {c50.threshold_:.2f}")
    
    if c50.missing_strategy_:
        print(f"  StratÃ©gie NaN: {c50.missing_strategy_['strategy']}")
        print(f"    - ProbabilitÃ© gauche: {c50.missing_strategy_['proba_left']:.2%}")
        print(f"    - ProbabilitÃ© droite: {c50.missing_strategy_['proba_right']:.2%}")
    
    # Test avec NaN
    X_test = np.array([[2.5, 25.0], [np.nan, np.nan], [5.5, 55.0]])
    y_pred = c50.predict(X_test)
    y_proba = c50.predict_proba(X_test)
    
    print(f"\n  ğŸ“‹ PrÃ©dictions sur donnÃ©es de test:")
    for i, (x, pred, proba) in enumerate(zip(X_test, y_pred, y_proba)):
        print(f"    {i+1}. {x} â†’ Classe {pred} (proba: {proba})")
    
    print("\nğŸ’¡ C5.0 utilise une distribution probabiliste pour les valeurs manquantes!")


def example_3_pruning():
    """Exemple 3: Ã‰lagage pessimiste."""
    print("\n" + "="*70)
    print("EXEMPLE 3: Ã‰LAGAGE PESSIMISTE")
    print("="*70 + "\n")
    
    # DonnÃ©es avec un peu de bruit
    np.random.seed(42)
    n = 50
    X = np.random.randn(n, 2)
    # Target avec bruit
    y = (X[:, 0] > 0).astype(int)
    # Ajouter 20% de bruit
    noise_idx = np.random.choice(n, size=n//5, replace=False)
    y[noise_idx] = 1 - y[noise_idx]
    
    print(f"ğŸ“Š Dataset avec bruit (~20% d'erreurs alÃ©atoires)")
    print(f"  Taille: {n} exemples")
    print(f"  Classes: {np.bincount(y)}\n")
    
    # Sans Ã©lagage
    print("ğŸ”µ SANS Ã‰LAGAGE:")
    print("â”€"*70)
    c50_no_prune = C50Stump(use_pruning=False)
    c50_no_prune.fit(X, y)
    print(f"  Accuracy: {c50_no_prune.score(X, y):.2%}")
    print(f"  Ã‰laguÃ©: {c50_no_prune.is_pruned_}")
    print(f"  Classes: gauche={c50_no_prune.left_class_}, droite={c50_no_prune.right_class_}")
    
    print()
    
    # Avec Ã©lagage
    print("ğŸŸ¢ AVEC Ã‰LAGAGE (confidence=0.25):")
    print("â”€"*70)
    c50_prune = C50Stump(use_pruning=True, confidence_level=0.25)
    c50_prune.fit(X, y)
    print(f"  Accuracy: {c50_prune.score(X, y):.2%}")
    print(f"  Ã‰laguÃ©: {c50_prune.is_pruned_}")
    print(f"  Classes: gauche={c50_prune.left_class_}, droite={c50_prune.right_class_}")
    
    if c50_prune.is_pruned_:
        print("\nğŸ’¡ Le stump a Ã©tÃ© Ã©laguÃ© en une feuille unique (classe majoritaire)")
        print("   car l'erreur estimÃ©e de la feuille est infÃ©rieure Ã  celle du stump.")


def example_4_cost_matrix():
    """Exemple 4: Matrice de coÃ»ts."""
    print("\n" + "="*70)
    print("EXEMPLE 4: MATRICE DE COÃ›TS D'ERREUR")
    print("="*70 + "\n")
    
    # Dataset mÃ©dical simulÃ©
    # Classe 0: Sain, Classe 1: Malade
    X = np.array([[1], [2], [3], [4], [5], [6], [7], [8]])
    y = np.array([0, 0, 0, 0, 1, 1, 1, 1])
    
    print("ğŸ“Š Cas mÃ©dical simulÃ©:")
    print("  - Classe 0: Patient sain")
    print("  - Classe 1: Patient malade\n")
    
    # Sans matrice de coÃ»ts
    print("ğŸ”µ SANS MATRICE DE COÃ›TS (coÃ»ts Ã©gaux):")
    print("â”€"*70)
    c50_equal = C50Stump()
    c50_equal.fit(X, y)
    print(f"  Seuil: {c50_equal.threshold_:.2f}")
    print(f"  Accuracy: {c50_equal.score(X, y):.2%}\n")
    
    # Avec matrice de coÃ»ts asymÃ©trique
    # Faux nÃ©gatif (dire sain alors que malade) coÃ»te trÃ¨s cher
    cost_matrix = np.array([
        [0, 1],   # Vrai sain â†’ Faux malade: coÃ»t 1
        [10, 0]   # Vrai malade â†’ Faux sain: coÃ»t 10 (DANGEREUX!)
    ])
    
    print("ğŸŸ¢ AVEC MATRICE DE COÃ›TS:")
    print("  Matrice:")
    print("           PrÃ©dit Sain  PrÃ©dit Malade")
    print("  Vrai Sain       0           1")
    print("  Vrai Malade    10           0")
    print()
    print("â”€"*70)
    c50_cost = C50Stump(cost_matrix=cost_matrix)
    c50_cost.fit(X, y)
    print(f"  Seuil: {c50_cost.threshold_:.2f}")
    print(f"  Score (1 - erreur pondÃ©rÃ©e): {c50_cost.score(X, y):.2%}")
    
    print("\nğŸ’¡ La matrice de coÃ»ts influence la sÃ©lection du seuil")
    print("   pour minimiser les erreurs coÃ»teuses (faux nÃ©gatifs).")


def example_5_comparison_benchmark():
    """Exemple 5: Benchmark complet."""
    print("\n" + "="*70)
    print("EXEMPLE 5: BENCHMARK COMPLET")
    print("="*70 + "\n")
    
    # GÃ©nÃ©rer dataset rÃ©aliste
    np.random.seed(42)
    n = 200
    X = np.random.randn(n, 5)
    y = (X[:, 0] + X[:, 2] > 0).astype(int)
    
    # Ajouter valeurs manquantes (10%)
    missing_mask = np.random.rand(n, 5) < 0.1
    X[missing_mask] = np.nan
    
    # Split train/test
    split = int(0.7 * n)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    print(f"ğŸ“Š Dataset:")
    print(f"  Train: {len(X_train)} exemples")
    print(f"  Test: {len(X_test)} exemples")
    print(f"  Features: 5")
    print(f"  Valeurs manquantes: ~10%\n")
    
    models = {
        'Decision Stump (Gini)': DecisionStump(criterion='gini'),
        'Decision Stump (Entropy)': DecisionStump(criterion='entropy'),
        'C5.0 Stump (basic)': C50Stump(handle_missing=False, use_pruning=False),
        'C5.0 Stump (full)': C50Stump(handle_missing=True, use_pruning=True)
    }
    
    print("="*70)
    print(f"{'ModÃ¨le':<30} {'Train Acc':<12} {'Test Acc':<12} {'Notes':<20}")
    print("="*70)
    
    for name, model in models.items():
        try:
            model.fit(X_train, y_train)
            train_acc = model.score(X_train, y_train)
            test_acc = model.score(X_test, y_test)
            
            notes = ""
            if isinstance(model, C50Stump):
                if model.is_pruned_:
                    notes += "[Ã‰LAGUÃ‰] "
                if model.missing_strategy_:
                    notes += "[NaN OK]"
            
            print(f"{name:<30} {train_acc:<12.2%} {test_acc:<12.2%} {notes:<20}")
            
        except Exception as e:
            print(f"{name:<30} {'ERREUR':<12} {'ERREUR':<12} {str(e)[:20]}")
    
    print("="*70)
    
    print("\nğŸ’¡ Observations:")
    print("  - C5.0 Stump (full) gÃ¨re les valeurs manquantes nativement")
    print("  - L'Ã©lagage peut amÃ©liorer la gÃ©nÃ©ralisation sur test")
    print("  - Gain Ratio Ã©vite le surapprentissage sur features bruitÃ©es")


def main():
    """Fonction principale."""
    print("\n" + "ğŸŒ³"*35)
    print(" "*15 + "DECISION STUMP vs C5.0 STUMP - COMPARAISON")
    print("ğŸŒ³"*35 + "\n")
    
    try:
        example_1_gain_ratio_bias()
        example_2_missing_values()
        example_3_pruning()
        example_4_cost_matrix()
        example_5_comparison_benchmark()
        
        print("\n\n" + "="*70)
        print("âœ… TOUS LES EXEMPLES TERMINÃ‰S AVEC SUCCÃˆS!")
        print("="*70)
        
        print("\nğŸ¯ RÃ‰SUMÃ‰ DES AMÃ‰LIORATIONS C5.0 STUMP:")
        print("  1. âœ… Gain Ratio corrige biais du Gain d'Information")
        print("  2. âœ… Gestion native des valeurs manquantes")
        print("  3. âœ… Ã‰lagage pessimiste pour meilleure gÃ©nÃ©ralisation")
        print("  4. âœ… Support de matrice de coÃ»ts asymÃ©trique")
        print("  5. âœ… Statistiques dÃ©taillÃ©es pour analyse")
        
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()